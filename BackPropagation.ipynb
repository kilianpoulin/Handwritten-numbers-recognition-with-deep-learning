{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUTS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "NB_HIDDEN_LAYERS = 1\n",
    "NB_HIDDEN_NODES = [300]\n",
    "NB_INPUTS = 784\n",
    "NB_OUTPUTS = len(OUTPUTS)\n",
    "NB_EPOCH = 20\n",
    "NB_DATA = 10000\n",
    "TRAINING_PATH = 'Training data/'\n",
    "TESTING_PATH = 'Testing data/'\n",
    "L_RATE = 1.0\n",
    "L_RATE_DESC = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def loadImage(img_path, number = 0):\n",
    "    pix = np.asarray(Image.open(img_path).convert('L'))\n",
    "    pix = ((pix * (1 / 2550) - 0.05)).reshape(784)\n",
    "    expected = []\n",
    "    if OUTPUTS != []:\n",
    "        for i in OUTPUTS:\n",
    "            if i == number:\n",
    "                expected = np.append(expected, 1)\n",
    "            else:\n",
    "                expected = np.append(expected, 0)\n",
    "    return pix, expected\n",
    "\n",
    "def load_training_dataset():\n",
    "    dataset = []\n",
    "    expected = []\n",
    "    \n",
    "    filenames = []\n",
    "    for o in OUTPUTS:\n",
    "        for filename in glob.glob(TRAINING_PATH + str(o) + '/*.png'): \n",
    "            base = os.path.basename(filename)\n",
    "            filenames.append([str(o) + '/' + base, o])\n",
    "    \n",
    "    random.shuffle(filenames)\n",
    "    if NB_DATA != 0:\n",
    "        filenames = filenames[:NB_DATA]\n",
    "        \n",
    "    for file in filenames:\n",
    "        img, exp = loadImage(TRAINING_PATH + file[0], file[1])\n",
    "        dataset.append(img)\n",
    "        expected.append(exp)\n",
    "        \n",
    "    print('end loading dataset')\n",
    "    return dataset, expected\n",
    "\n",
    "def load_dataset(path):\n",
    "    dataset = []\n",
    "    filenames = []\n",
    "    for filename in glob.glob(path + '/*.png'): \n",
    "        dataset.append(loadImage(filename))\n",
    "        base = os.path.splitext(os.path.basename(filename))[0]\n",
    "        filenames.append(base)\n",
    "     \n",
    "    return len(dataset), filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    predict_o = []\n",
    "    filenames = []\n",
    "    correct = 0\n",
    "    for number in OUTPUTS:\n",
    "        for filename in glob.glob(TESTING_PATH + str(number) + '/*.png'):\n",
    "            base = os.path.splitext(os.path.basename(filename))[0]\n",
    "            data = loadImage(filename)\n",
    "            filenames.append(base)\n",
    "            \n",
    "            output = forward_prop(data[0])\n",
    "\n",
    "            val = [i for i in np.where(output == np.amax(output))]\n",
    "            ans = np.around(OUTPUTS[val[0][0]]).astype(int)\n",
    "            #print(val)\n",
    "            if ans == number:\n",
    "                correct += 1\n",
    "            predict_o.append(ans)\n",
    "            \n",
    "    final = list()\n",
    "\n",
    "    for i in range(len(filenames)):\n",
    "        final.append({ filenames[i] : predict_o[i]})\n",
    "\n",
    "\n",
    "    with open('outputs.txt', 'w') as f:\n",
    "        for item in final:\n",
    "            f.write(\"%s\\n\" % str(item).replace('{', '').replace('}', '').replace(\"'\", '').replace(':', ''))\n",
    "            \n",
    "    print('correct : ' + str(correct) + ' (' + str((correct / len(filenames)) * 100) + ' %)')\n",
    "    print('wrong : ' + str(len(filenames) - correct) + ' (' + str(((len(filenames) - correct) / len(filenames)) * 100) + ' %)')\n",
    "    #return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrain(data_type, data_nb, nb_iter = 0, outputs = []):\n",
    "\n",
    "    nb_outputs = 0\n",
    "    nb_inputs = 0\n",
    "    \n",
    "    #for images\n",
    "    if data_type == 'img':\n",
    "        nb_inputs = 28 * 28\n",
    "        nb_outputs = len(outputs)\n",
    "        x = np.array(load_training_dataset(outputs, data_nb), dtype=np.dtype(Decimal))\n",
    "        x = [[ Decimal(i) for i in j] for j in x]\n",
    "    elif data_type == 'xor':\n",
    "        nb_outputs = 3\n",
    "        nb_inputs = 2\n",
    "        x = [[1, 0, 1, 1, 1], [1, 1, 0, 1, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 1]]\n",
    "    else:\n",
    "        return 'Wrong datatype (should be img or xor)'\n",
    "\n",
    "\n",
    "    return train_network(network, x, 1, nb_outputs, nb_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    #print(1 / (1 + np.exp(-x)))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    " \n",
    "def error_function(expected, outputs):\n",
    "   # return (1/len(outputs)) * sum(np.square(expected - outputs))\n",
    "    return sum(np.square(expected - outputs))\n",
    "\n",
    "def forward_prop(inputs):\n",
    "    global network\n",
    "    vect_sigmoid = np.vectorize(sigmoid)\n",
    "    \n",
    "    for i in range(len(network)):\n",
    "        layer = network[i]\n",
    "        new_inputs = []\n",
    "        \n",
    "        if (inputs.shape)[0] == 1 :\n",
    "            layer[0]['a'] = vect_sigmoid(np.dot(layer[1]['w'], inputs.T))\n",
    "        else:\n",
    "            layer[0]['a'] = vect_sigmoid(np.dot(layer[1]['w'], inputs))\n",
    "\n",
    "        new_inputs.append(layer[0]['a'])\n",
    "        inputs = new_inputs[0]\n",
    "    return np.matrix(inputs)\n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(data):\n",
    "    delta = []\n",
    "    global L_RATE\n",
    "    # print('update l rate : ' + str(L_RATE))\n",
    "    #  print('len network : ' + str(len(network)))\n",
    "    for i in range(len(network)):\n",
    "        layer = network[i]\n",
    "        \n",
    "        if i != 0:\n",
    "            delta = np.multiply((layer[3]['e'] * L_RATE).T, network[i - 1][0]['a'])\n",
    "        else:\n",
    "            delta = np.multiply((layer[3]['e'] * L_RATE).T, data)\n",
    "        layer[1]['w'] = layer[1]['w'] + delta.T\n",
    "        \"\"\"\n",
    "        for neuron in range(len(layer[1]['w'])):\n",
    "            for b_neuron in range(len(layer[1]['w'][neuron])):\n",
    "                if i != 0:\n",
    "                    delta = l_rate * layer[3]['e'][neuron] * network[i - 1][0]['a'][b_neuron]\n",
    "                else:\n",
    "                    delta = l_rate * layer[3]['e'][neuron] * data[b_neuron]\n",
    "                layer[1]['w'][neuron][b_neuron] = layer[1]['w'][neuron][b_neuron] + delta\n",
    "        \"\"\"\n",
    "        \n",
    "def backward_prop_error(expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = []\n",
    "        \n",
    "        # case where it is not the output layer\n",
    "        if i != len(network)-1:\n",
    "            o_errors = network[i + 1][3]['e']\n",
    "            o_weights = network[i + 1][1]['w']\n",
    "            \n",
    "            errors = np.dot(\n",
    "                np.dot(o_weights.T, o_errors),\n",
    "                np.dot((layer[0]['a'].T), 1 - layer[0]['a']))\n",
    "            \n",
    "            layer[3]['e'] = errors\n",
    "        else:\n",
    "            errors = np.dot(expected.T - layer[0]['a'], np.dot((layer[0]['a']).T, (1 - layer[0]['a'])))\n",
    "            layer[3]['e']  = errors\n",
    "            \n",
    "def update_line(hl, new_data, iter):\n",
    "    hl.set_xdata(np.append(hl.get_xdata(), iter))\n",
    "    hl.set_ydata(np.append(hl.get_ydata(), new_data))\n",
    "    plt.draw()\n",
    "    \n",
    "\n",
    "def train_network(train, expected):\n",
    "    \n",
    "    global network\n",
    "    global L_RATE\n",
    "    global L_RATE_DESC\n",
    "    print('l rate = ' + str(L_RATE))\n",
    "    err_plot, = plt.plot([], [])\n",
    "    plt.ylabel('error')\n",
    "    axes = plt.gca()\n",
    "    iter = 0\n",
    "    for epoch in range(NB_EPOCH):\n",
    "        error = 0\n",
    "        iter = 0\n",
    "        for data in train:\n",
    "            new_data = data.T\n",
    "            outputs = forward_prop(new_data)\n",
    "            \n",
    "            error += error_function(expected[iter].T, outputs)\n",
    "            \n",
    "            backward_prop_error(expected[iter])\n",
    "            update_weights(new_data)\n",
    "            \n",
    "            iter += 1\n",
    "            \n",
    "        final_err = error / len(train)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, L_RATE, final_err))\n",
    "        \n",
    "        L_RATE *= L_RATE_DESC\n",
    "        if epoch == 0:\n",
    "            axes.set_xlim([0, NB_EPOCH - 1])\n",
    "            axes.set_ylim([0, float(final_err) + 0.1])\n",
    "            \n",
    "        update_line(err_plot, final_err, epoch)\n",
    "        \n",
    "        if error < 0.0001:\n",
    "            print('error : ' + str(final_err))\n",
    "            return network\n",
    "    print('error     : ' + str(final_err))\n",
    "    return network\n",
    "   \n",
    "\n",
    "def initialize_network():\n",
    "    \n",
    "    global network\n",
    "    w = []\n",
    "    \n",
    "    # additionnal hidden layers\n",
    "    for i in range(NB_HIDDEN_LAYERS):\n",
    "        neurons = [{ 'neurons': NB_HIDDEN_NODES[i]}]\n",
    "        if i == 0:\n",
    "            w.append(np.random.uniform(low=-0.5, high=0.5, size=(NB_HIDDEN_NODES[i], NB_INPUTS)))\n",
    "            #w.append(np.matrix(np.random.rand(nb_hidden[i], nb_inputs)) / 1000) \n",
    "        else:\n",
    "            w.append(np.random.uniform(low=-0.9, high=0.9, size=(NB_HIDDEN_NODES[i], NB_HIDDEN_NODES[i - 1])))\n",
    "            #w.append(np.matrix(np.random.rand(nb_hidden[i], nb_hidden[i - 1])) / 100)\n",
    "        network.append(neurons)\n",
    "    \n",
    "    # output layer\n",
    "    neurons = [{ 'neurons': NB_OUTPUTS}]\n",
    "    w.append(np.random.uniform(low=-0.5, high=0.5, size=(NB_OUTPUTS, NB_HIDDEN_NODES[NB_HIDDEN_LAYERS - 1])))\n",
    "    #w.append(np.matrix(np.random.rand(nb_outputs, nb_hidden[nb_hidden_layers - 1])))\n",
    "    network.append(neurons)\n",
    "    \n",
    "    for layer in network:\n",
    "              \n",
    "        wTmp = {'w': w[0]}\n",
    "        layer.append(wTmp)\n",
    "        w.pop(0)\n",
    "        \n",
    "        aTmp = { 'a': np.matrix(np.random.rand(layer[0]['neurons']))}\n",
    "        layer.append(aTmp)\n",
    "        \n",
    "        eTmp = { 'e': np.matrix(np.random.rand(layer[0]['neurons']))}\n",
    "        layer.append(eTmp)\n",
    "        \n",
    "    print('end initializing network')\n",
    "    return network\n",
    "  \n",
    "    \n",
    "# data_type => either 'img' or 'xor'\n",
    "# data_nb => number of data rows (entries)\n",
    "# nb_iter => number of iterations ot get to result?\n",
    "# outputs => list of outputs (from 0 to 9 for img), not necessary for XOR\n",
    "# nb_hidden layer => number of hidden layers, 1 by default\n",
    "# nb_hidden => number of hidden neurons on each hidden layer ( 2 by default )\n",
    "\n",
    "\n",
    "def train(data_type):\n",
    "\n",
    "    global network\n",
    "    global L_RATE\n",
    "    expected = []\n",
    "    dataset = []\n",
    "    #for imagesTra\n",
    "    if data_type == 'img':\n",
    "        if dataset == []:\n",
    "            dataset, exp = load_training_dataset()\n",
    "            x = np.matrix(dataset)\n",
    "            expected = np.matrix(exp)\n",
    "        else:\n",
    "            x = dataset\n",
    "    elif data_type == 'xor':\n",
    "        x = np.matrix([[1, 0, 1, 1, 1], [1, 1, 0, 1, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 1]])\n",
    "    else:\n",
    "        return 'Wrong datatype (should be iprintpmg or xor)'\n",
    "\n",
    "    network = initialize_network()\n",
    "    return train_network(x, expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end loading dataset\n",
      "end initializing network\n",
      "l rate = 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test(network, [[1,0], [1,1], [0,0], [0,1]], 4)\n",
    "\n",
    "#dataset = np.matrix(load_training_dataset([0,1, 3], 10))\n",
    "#print('done')#network = retrain('img', 5000, 10, [0,1,3, 8, 9])\n",
    "#test_dataset, filenames = load_testing_dataset('Testing data')\n",
    "#test(network, test_dataset, 15, [0,1, 2, 3, 4, 5, 6, 7, 8, 9], filenames)\n",
    "network = list()\n",
    "#network = train('xor', 4, 100, [], [16, 8], 2)\n",
    "network = train('img')\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
